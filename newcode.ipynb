{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!pip install matplotlib scikit-learn transformers pandas newscatcherapi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from newscatcherapi import NewsCatcherApiClient\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from transformers import pipeline, T5Tokenizer, T5ForConditionalGeneration\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, ConfusionMatrixDisplay, classification_report\n",
    "import pandas as pd\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Initialize NewsCatcher API client with your API key\n",
    "newscatcherapi = NewsCatcherApiClient(x_api_key='pcbPN7A0HzJquQTcEFjBM6L0LK0n2D7tAZIRb-Dsb-c')\n",
    "query = \"bitcoin\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Fetch Articles from NewsCatcher API\n",
    "all_articles = newscatcherapi.get_search(\n",
    "    q=query,\n",
    "    lang='en',\n",
    "    search_in='title',\n",
    "    from_='1 days ago',\n",
    "    countries='US',\n",
    "    page_size=100,\n",
    "    topic='finance',\n",
    "    sort_by='rank',\n",
    "    page=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Extract unique articles by their summaries\n",
    "try:\n",
    "    unique_articles = set(article['summary'] for article in all_articles['articles'])\n",
    "except KeyError:\n",
    "    print(\"Error: 'summary' not found in articles.\")\n",
    "    unique_articles = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Load Sentiment Analysis model (FinBERT)\n",
    "model_name = \"ProsusAI/finbert\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertForSequenceClassification.from_pretrained(model_name)\n",
    "nlp = pipeline(\"sentiment-analysis\", model=model, tokenizer=tokenizer, truncation=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Analyze sentiment of each article\n",
    "sentiment_scores = []\n",
    "sentiment_labels = []\n",
    "num_positive, num_negative, num_neutral = 0, 0, 0\n",
    "article_predictions = []  # To store article summaries and their predictions\n",
    "\n",
    "for article in unique_articles:\n",
    "    try:\n",
    "        pred = nlp(article)[0]\n",
    "        sentiment_scores.append(pred['score'])\n",
    "        sentiment_labels.append(pred['label'])\n",
    "        article_predictions.append([article, pred['label']])\n",
    "        print(f\"Article: {article}\\nPrediction: {pred}\\n\")\n",
    "        if pred['label'] == 'positive':\n",
    "            num_positive += 1\n",
    "        elif pred['label'] == 'negative':\n",
    "            num_negative += 1\n",
    "        else:\n",
    "            num_neutral += 1\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing article: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Calculate Stability Index and other statistics\n",
    "stability_index = sum(\n",
    "    score if label == 'positive' else -score\n",
    "    for score, label in zip(sentiment_scores, sentiment_labels)\n",
    ") / len(sentiment_scores) if sentiment_scores else 0\n",
    "\n",
    "mean_score = np.mean(sentiment_scores) if sentiment_scores else 0\n",
    "std_deviation = np.std(sentiment_scores) if sentiment_scores else 0\n",
    "\n",
    "# Print summary statistics\n",
    "print(f\"Stability Index: {stability_index}\")\n",
    "print(f\"Mean Sentiment Score: {mean_score}\")\n",
    "print(f\"Sentiment Standard Deviation: {std_deviation}\")\n",
    "print(f\"Positive Articles: {num_positive}\")\n",
    "print(f\"Negative Articles: {num_negative}\")\n",
    "print(f\"Neutral Articles: {num_neutral}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Visualize the distribution of sentiment labels\n",
    "def plot_sentiment_distribution(labels):\n",
    "    counts = Counter(labels)\n",
    "    labels, values = zip(*counts.items())\n",
    "    plt.bar(labels, values, color=['green', 'red', 'blue'])\n",
    "    plt.xlabel(\"Sentiment\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.title(\"Sentiment Distribution\")\n",
    "    plt.savefig('sentiment_distribution.png')  # Save the chart\n",
    "    plt.show()\n",
    "\n",
    "plot_sentiment_distribution(sentiment_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Visualize the distribution of sentiment scores\n",
    "def plot_sentiment_scores(scores):\n",
    "    plt.hist(scores, bins=10, color='purple', alpha=0.7)\n",
    "    plt.xlabel(\"Sentiment Score\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.title(\"Distribution of Sentiment Scores\")\n",
    "    plt.savefig('sentiment_scores.png')  # Save the chart\n",
    "    plt.show()\n",
    "\n",
    "plot_sentiment_scores(sentiment_scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Pie chart for sentiment proportions\n",
    "labels = ['Positive', 'Negative', 'Neutral']\n",
    "sizes = [num_positive, num_negative, num_neutral]\n",
    "colors = ['green', 'red', 'blue']\n",
    "plt.pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%', startangle=140)\n",
    "plt.title(\"Sentiment Proportions\")\n",
    "plt.savefig('sentiment_proportions.png')  # Save the chart\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Topic extraction from articles using TF-IDF\n",
    "try:\n",
    "    tfidf = TfidfVectorizer(max_features=10)\n",
    "    features = tfidf.fit_transform(unique_articles)\n",
    "    print(\"Top Topics:\", tfidf.get_feature_names_out())\n",
    "except ValueError as e:\n",
    "    print(\"Error in topic extraction:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Summarize the articles using T5\n",
    "summarizer_model_name = \"t5-small\"\n",
    "summarizer_tokenizer = T5Tokenizer.from_pretrained(summarizer_model_name)\n",
    "summarizer_model = T5ForConditionalGeneration.from_pretrained(summarizer_model_name)\n",
    "\n",
    "for article in unique_articles:\n",
    "    article = \"summarize: \" + article\n",
    "    inputs = summarizer_tokenizer.encode(article, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "    outputs = summarizer_model.generate(\n",
    "        inputs,\n",
    "        max_length=50,\n",
    "        min_length=15,\n",
    "        length_penalty=2.0,\n",
    "        num_beams=5,\n",
    "        early_stopping=True\n",
    "    )\n",
    "    summary = summarizer_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    print(\"Summary:\", summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Save predictions to a .tsv file\n",
    "df = pd.DataFrame(article_predictions, columns=[\"Article\", \"Sentiment\"])\n",
    "df.to_csv('article_predictions.tsv', sep='\\t', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Test data and corresponding labels for evaluation\n",
    "test_data = [\n",
    "    \"The stock market is hitting new highs, and investors are optimistic about the future.\",\n",
    "    \"The new healthcare bill is a disaster and will lead to severe consequences for millions.\",\n",
    "    \"This movie is a masterpiece, with brilliant performances from the entire cast.\",\n",
    "    \"The economy is in turmoil, and it seems like there is no hope for recovery.\",\n",
    "    \"The tech industry is booming, with new innovations emerging every day.\",\n",
    "    \"Itâ€™s a sad day for the company, as the CEO announced massive layoffs.\",\n",
    "    \"The team's victory was incredible, showing their true strength and resilience.\",\n",
    "    \"The political situation is becoming increasingly unstable, with widespread protests across the country.\",\n",
    "    \"The weather today is neither hot nor cold, itâ€™s just perfectly neutral.\",\n",
    "    \"I can't believe how bad the customer service was today, completely frustrating!\",\n",
    "    \"The recent advancements in AI are nothing short of revolutionary, changing the landscape of technology.\",\n",
    "    \"Despite all the challenges, the organization managed to deliver exceptional results this quarter.\",\n",
    "    \"The new social media app has some unique features, but it's a bit difficult to use at first.\",\n",
    "    \"There has been a significant improvement in the companyâ€™s financial performance this year.\",\n",
    "    \"The new restaurant in town has great food but terrible service.\",\n",
    "    \"I feel indifferent about the movie; it was neither good nor bad.\",\n",
    "    \"The president's speech was inspiring and lifted the spirits of the nation.\",\n",
    "    \"There's been a major setback in the project, but the team is working hard to overcome it.\",\n",
    "    \"Although the market is struggling, there are still opportunities for savvy investors.\",\n",
    "    \"The new smartphone model has great features, but it's too expensive for most people.\"\n",
    "]\n",
    "\n",
    "test_labels = [\n",
    "    \"positive\", \"negative\", \"positive\", \"negative\", \"positive\", \"negative\", \n",
    "    \"positive\", \"negative\", \"neutral\", \"negative\", \"positive\", \"positive\", \n",
    "    \"neutral\", \"positive\", \"negative\", \"neutral\", \"positive\", \"negative\", \n",
    "    \"positive\", \"negative\"\n",
    "]\n",
    "\n",
    "# Predictions from the model\n",
    "predicted_labels = [nlp(text)[0]['label'] for text in test_data]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Evaluate performance on test data\n",
    "accuracy = accuracy_score(test_labels, predicted_labels)\n",
    "precision = precision_score(test_labels, predicted_labels, average='weighted')\n",
    "recall = recall_score(test_labels, predicted_labels, average='weighted')\n",
    "f1 = f1_score(test_labels, predicted_labels, average='weighted')\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1 Score: {f1}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "conf_matrix = confusion_matrix(test_labels, predicted_labels, labels=[\"positive\", \"negative\", \"neutral\"])\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=conf_matrix, display_labels=[\"positive\", \"negative\", \"neutral\"])\n",
    "disp.plot(cmap=plt.cm.Blues)\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.savefig('confusion_matrix.png')  # Save the chart\n",
    "plt.show()\n",
    "\n",
    "# Classification Report\n",
    "report = classification_report(test_labels, predicted_labels, labels=[\"positive\", \"negative\", \"neutral\"])\n",
    "print(\"Classification Report:\\n\", report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Plot the distribution of the classes in test data\n",
    "def plot_class_distribution(labels):\n",
    "    counts = Counter(labels)\n",
    "    labels, values = zip(*counts.items())\n",
    "    plt.bar(labels, values, color=['green', 'red', 'blue'])\n",
    "    plt.xlabel(\"Class\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.title(\"Class Distribution\")\n",
    "    plt.savefig('class_distribution.png')  # Save the chart\n",
    "    plt.show()\n",
    "\n",
    "plot_class_distribution(test_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Cross-validation with RandomForestClassifier\n",
    "X_train, X_test, y_train, y_test = train_test_split(test_data, test_labels, test_size=0.2, random_state=42)\n",
    "cross_val_scores = cross_val_score(RandomForestClassifier(), X_train, y_train, cv=5)\n",
    "print(f\"Cross-validation scores: {cross_val_scores}\")\n",
    "print(f\"Mean Cross-validation score: {np.mean(cross_val_scores)}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
